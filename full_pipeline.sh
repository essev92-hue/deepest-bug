#!/bin/bash
# full_pipeline.sh - Complete bug hunting pipeline with 7 tools

DOMAIN=$1
if [ -z "$DOMAIN" ]; then
    echo "Usage: $0 <domain>"
    exit 1
fi

echo "üöÄ COMPLETE BUG HUNTING PIPELINE - 7 TOOLS"
echo "=========================================="
echo "Target: $DOMAIN"
echo "Start time: $(date)"
echo ""

# Create necessary directories
mkdir -p targets/{recon,scan,results} logs reports

# 1. SUBFINDER
echo "[1/7] üîç Subfinder: Subdomain enumeration..."
./tools/subfinder -d $DOMAIN -silent -o targets/recon/subfinder_$DOMAIN.txt

# 2. ASSETFINDER
echo "[2/7] üîç Assetfinder: Additional assets..."
./tools/assetfinder --subs-only $DOMAIN > targets/recon/assetfinder_$DOMAIN.txt

# Merge results
cat targets/recon/*_$DOMAIN.txt | sort -u > targets/recon/all_subs.txt
SUBS_COUNT=$(wc -l < targets/recon/all_subs.txt)
echo "‚úÖ Found $SUBS_COUNT unique subdomains"

# 3. HTTPX - Find live hosts
echo "[3/7] üåê HTTPX: Finding live hosts..."
./tools/httpx -l targets/recon/all_subs.txt -silent -threads 20 \
    -ports 80,443,8080,8443,3000 -o targets/recon/live_hosts.txt
LIVE_COUNT=$(wc -l < targets/recon/live_hosts.txt 2>/dev/null || echo 0)
echo "‚úÖ Found $LIVE_COUNT live hosts"

# 4. KATANA - Crawling (FIXED PARAMETERS)
echo "[4/7] üï∑Ô∏è Katana: Crawling URLs..."
if [ $LIVE_COUNT -gt 0 ]; then
    # Fixed katana parameters
    cat targets/recon/live_hosts.txt | ./tools/katana \
        -d 2 \
        -jc \
        -kf all \
        -c 10 \
        -silent \
        -o targets/scan/crawled_urls.txt
    
    if [ $? -eq 0 ] && [ -f "targets/scan/crawled_urls.txt" ]; then
        URLS_COUNT=$(wc -l < targets/scan/crawled_urls.txt)
        echo "‚úÖ Crawled $URLS_COUNT URLs"
    else
        echo "‚ö†Ô∏è  Katana crawling failed or no URLs found"
        URLS_COUNT=0
        touch targets/scan/crawled_urls.txt
    fi
else
    echo "‚ö†Ô∏è  No live hosts to crawl"
    URLS_COUNT=0
    touch targets/scan/crawled_urls.txt
fi

# 5. NUCLEI - Vulnerability scanning
echo "[5/7] üéØ Nuclei: Vulnerability scanning..."
if [ $LIVE_COUNT -gt 0 ]; then
    echo "   Scanning with nuclei (medium+ severity)..."
    ./tools/nuclei -l targets/recon/live_hosts.txt \
        -severity medium,high,critical \
        -etags intrusive \
        -rate-limit 30 \
        -c 10 \
        -silent \
        -o targets/results/nuclei_$DOMAIN.txt
    
    if [ -f "targets/results/nuclei_$DOMAIN.txt" ]; then
        NUCLEI_COUNT=$(wc -l < targets/results/nuclei_$DOMAIN.txt)
        echo "‚úÖ Found $NUCLEI_COUNT potential vulnerabilities"
        
        # Show top findings
        if [ $NUCLEI_COUNT -gt 0 ]; then
            echo "   Top findings:"
            head -3 targets/results/nuclei_$DOMAIN.txt | while read line; do
                echo "   ‚Ä¢ $(echo $line | cut -d' ' -f1)"
            done
        fi
    else
        echo "‚úÖ No critical vulnerabilities found"
        NUCLEI_COUNT=0
    fi
else
    echo "‚ö†Ô∏è  No live hosts to scan"
    NUCLEI_COUNT=0
fi

# 6. DALFOX - XSS scanning
echo "[6/7] ‚ú® Dalfox: XSS scanning..."
if [ -f "targets/scan/crawled_urls.txt" ] && [ $URLS_COUNT -gt 0 ]; then
    # Extract URLs with parameters
    grep "?" targets/scan/crawled_urls.txt > targets/scan/param_urls.txt 2>/dev/null
    PARAM_COUNT=$(wc -l < targets/scan/param_urls.txt 2>/dev/null || echo 0)
    
    if [ $PARAM_COUNT -gt 0 ]; then
        echo "   Testing $PARAM_COUNT parameterized URLs..."
        ./tools/dalfox file targets/scan/param_urls.txt \
            --skip-bav \
            --only-custom-payload \
            --silence \
            -o targets/results/xss_$DOMAIN.txt
        
        if [ -f "targets/results/xss_$DOMAIN.txt" ]; then
            XSS_COUNT=$(wc -l < targets/results/xss_$DOMAIN.txt)
            echo "‚úÖ Found $XSS_COUNT XSS issues"
        else
            echo "‚úÖ No XSS vulnerabilities found"
            XSS_COUNT=0
        fi
    else
        echo "‚ö†Ô∏è  No parameter URLs found for XSS testing"
        XSS_COUNT=0
    fi
else
    echo "‚ö†Ô∏è  No crawled URLs for XSS testing"
    XSS_COUNT=0
fi

# 7. FFUF - Directory fuzzing (limited)
echo "[7/7] üîé FFUF: Quick directory fuzzing..."
if [ -f "targets/recon/live_hosts.txt" ] && [ $LIVE_COUNT -gt 0 ]; then
    mkdir -p targets/results/ffuf/
    counter=0
    
    # Use common wordlist or create minimal one
    if [ ! -f "wordlists/common.txt" ]; then
        echo "   Creating minimal wordlist..."
        mkdir -p wordlists
        echo -e "admin\napi\ndashboard\nlogin\nwp-admin\nphpmyadmin\ntest\ndev" > wordlists/common.txt
    fi
    
    echo "   Scanning top 2 hosts..."
    while read url && [ $counter -lt 2 ]; do
        echo "     ‚Üí $url"
        domain_clean=$(echo $url | sed 's|[^a-zA-Z0-9]|_|g')
        
        ./tools/ffuf -u $url/FUZZ \
            -w wordlists/common.txt \
            -t 5 \
            -rate 5 \
            -timeout 3 \
            -mc 200,301,302,403 \
            -o targets/results/ffuf/${domain_clean}.json \
            -of json \
            -quiet 2>/dev/null
        
        if [ -f "targets/results/ffuf/${domain_clean}.json" ]; then
            results=$(jq '.results | length' targets/results/ffuf/${domain_clean}.json 2>/dev/null || echo 0)
            echo "       Found $results directories"
        fi
        
        counter=$((counter + 1))
        sleep 1
    done < <(head -2 targets/recon/live_hosts.txt)
    
    echo "‚úÖ Directory fuzzing completed"
else
    echo "‚ö†Ô∏è  No live hosts for directory fuzzing"
fi

echo ""
echo "=========================================="
echo "üéâ PIPELINE COMPLETED!"
echo "‚è∞ End time: $(date)"
echo ""
echo "üìä RESULTS SUMMARY:"
echo "   ‚Ä¢ Subdomains found: $SUBS_COUNT"
echo "   ‚Ä¢ Live hosts: $LIVE_COUNT"
echo "   ‚Ä¢ Crawled URLs: $URLS_COUNT"
echo "   ‚Ä¢ Nuclei findings: $NUCLEI_COUNT"
echo "   ‚Ä¢ XSS findings: $XSS_COUNT"
echo ""
echo "üìÅ Output locations:"
echo "   ‚Ä¢ Recon data: targets/recon/"
echo "   ‚Ä¢ Crawled URLs: targets/scan/crawled_urls.txt"
echo "   ‚Ä¢ Findings: targets/results/"
echo ""
echo "üîç Next steps:"
echo "   ‚Ä¢ Review nuclei findings: cat targets/results/nuclei_$DOMAIN.txt"
echo "   ‚Ä¢ Check for sensitive files in crawled URLs"
echo "   ‚Ä¢ Manual testing on interesting endpoints"
echo "=========================================="
